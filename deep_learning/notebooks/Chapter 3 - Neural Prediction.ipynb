{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Introduction to Neural Prediction\n",
    "## Forward Propagation\n",
    "\n",
    "### TOC\n",
    "- Simple network making prediction\n",
    "- What Neural Nets are and what they do\n",
    "- Making predictions with multiple inputs\n",
    "- Making predictions with multiple outputs\n",
    "- Making predictions with multiple inputs and outputs\n",
    "- Predicting on predictions\n",
    "\n",
    "## Step 1: Predict\n",
    "\n",
    "In the previous chapter, we learned the paradigm: 'Predict, Compare, Learn'. This chapter is all about part 1. \n",
    "\n",
    "We also learned that the predict step looks a lot like: \n",
    "\n",
    "> Data --> Machine --> Prediction\n",
    "\n",
    "We'll start with only one input / predicting one datapoint at a time, like so: \n",
    "\n",
    "> \\# of toes --> Machine --> Prediction (98%)\n",
    "\n",
    "Later, we'll explore how our predictions are affected by the number of datapoints at a time we pass in. For example, predicting if a picture is a cat with one pixel at a time won't be accurate at all. You'd need all the pictures in order to make a real prediction! Good general rule of thumb is 'Always present enough information to the network'. Enough information is defined loosely as how much info a human would need to make the same prediction. \n",
    "\n",
    "We can only create our network once we understand the 'shape' of our input / output datasets. Shape means number of columns / number of datapoints we are processing at once. For now, our input is the one datapoint _# of toes_ and our output is the single prediction _likelihood the team will win_. Since we have only one of each in / out, we will have one knob (these are also called weights). Our network looks like \n",
    "\n",
    "> [input] -- # of toes --> weight 1 --> win? -- [output]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple NN making a prediction\n",
    "\n",
    "Starting with the simplest NN possible (our NN from above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.85\n"
     ]
    }
   ],
   "source": [
    "weight = 0.1\n",
    "def neural_network(input, weight):\n",
    "    prediction = input * weight\n",
    "    return prediction\n",
    "\n",
    "# Now let's feed in an input point: \n",
    "number_of_toes = [8.5, 9.5, 10, 9]\n",
    "input = number_of_toes[0]\n",
    "pred = neural_network(input, weight)\n",
    "print pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wooo first NN. \n",
    "\n",
    "Few important questions answered by above: \n",
    "- What is input data? \n",
    "    - number that we recorded in real world somewhere, like temperature, batting average, or stock price. \n",
    "- What is a prediction? \n",
    "    - What NN tells us given our input data, like given Temp, X% chance people wear sweaters today. \n",
    "- Is this prediction right? \n",
    "    - Not always, sometimes makes mistakes, just a matter of adjusting weights to make it more accurate. \n",
    "- How does this network learn? \n",
    "    - Trial and error. Makes prediction, adjusts weights based on outcome!\n",
    "- When NNs get the next set of input data, they forgot the last set of inputs! \n",
    "    - Not always the case, but for now we don't have any memory, as well as any back propagation. \n",
    "- What is a weight? \n",
    "    - Think of a weight as a measure of sensitivity to inputs from this channel. \n",
    "    - Or Volume? Bigger weights mean that the input is louder / has more influence on final prediction. \n",
    "\n",
    "## Making a Prediction with multiple inputs\n",
    "> NNs can combine intelligence from multiple datapoints. \n",
    "\n",
    "One datapoint at a time sometimes doesn't make for a very accurate prediction. In that case we can make a new network, or graph, but this time with 3 inputs, each edge with it's own weight. \n",
    "\n",
    "```\n",
    "          # toes - - - (0.1) - \n",
    "        /                      \\\n",
    "      /                         \\\n",
    "  Input - Win / Loss - - (.2) - - Win? --> (Some %)\n",
    "      \\                         /\n",
    "        \\                      /\n",
    "          # fans - - - (0) - -\n",
    "```\n",
    "\n",
    "Now let's transcribe this in code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.98\n"
     ]
    }
   ],
   "source": [
    "weights = [0.1, 0.2, 0]\n",
    "def neural_network(input, weights):\n",
    "    pred = w_sum(input, weights)\n",
    "    return pred\n",
    "\n",
    "def w_sum(a,b):\n",
    "    assert(len(a) == len(b))\n",
    "    output = 0\n",
    "    for i in range(len(a)):\n",
    "        output += (a[i] * b[i])\n",
    "    return output\n",
    "# Now if we pass in a dataset with stats from the last 4 games\n",
    "# toes = current # of toes\n",
    "# wlrec = games on (percent)\n",
    "# nfans = fan count (in million)\n",
    "\n",
    "toes = [8.5, 9.5, 9.9, 9.0]\n",
    "wlrec = [0.65, 0.8, 0.8, 0.9]\n",
    "nfans = [1.2, 1.3, 0.5, 1.0]\n",
    "\n",
    "# lets only deal with first game's data\n",
    "input = [toes[0], wlrec[0], nfans[0]]\n",
    "pred = neural_network(input, weights)\n",
    "print pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Inputs - What does this Neural Net do? \n",
    "\n",
    "With the new neural net defined above, we can accept multiple inputs at a time per prediction. Now the network can make even more informed decisions. However, the fundamental logic of the network has not changed. We still multiply each input by it's weight, and sum the result.\n",
    "\n",
    "Our input has now transformed into a **vector**. Vector is just a list of numbers. \n",
    "\n",
    "Vectors are incredibly useful when performing operations with groups of numbers. In our case, we want to perform a weighted sum. We do this by multiplying each number based on its position. Whenever we perform an operation like this, we call it an _element-wise_ operation. \n",
    "\n",
    "#### Challenge: Vector Math: \n",
    "Write 4 functions: elementwise_mult / add, vector_sum / average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element-mult [3, 8, 15]\n",
      "Element-add [4, 6, 8]\n",
      "Vector add 6\n",
      "Vector average 2.0\n",
      "Dot prod: 26\n"
     ]
    }
   ],
   "source": [
    "def elementwise_mult(a, b):\n",
    "    assert(len(a) == len(b))\n",
    "    output = [a[i] * b[i] for i in range(len(a))]\n",
    "    return output\n",
    "\n",
    "def elementwise_add(a, b):\n",
    "    assert(len(a) == len(b))\n",
    "    output = [a[i] + b[i] for i in range(len(a))]\n",
    "    return output\n",
    "\n",
    "def vector_sum(a):\n",
    "    return reduce((lambda x, y: x + y), a)\n",
    "\n",
    "def vector_average(a):\n",
    "    return float(vector_sum(a)) / len(a)\n",
    "\n",
    "a = [1,2,3]\n",
    "b = [3,4,5]\n",
    "\n",
    "print \"Element-mult %s\" % elementwise_mult(a,b)\n",
    "print \"Element-add %s\" % elementwise_add(a,b)\n",
    "print \"Vector add %s\" % vector_sum(a)\n",
    "print \"Vector average %s\" % vector_average(a)\n",
    "\n",
    "# perform a dot product by using element mult + vector sum\n",
    "print \"Dot prod: %s\" % (vector_sum(elementwise_mult(a,b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dot product can also be thought of a notion of _similarity_ between two vectors. The highest weighted sum (dot product) between two vectors would be between itself. Some have also equated dot product to an 'AND' statement of sorts. For a binary set of arrays a = [0,1] and b = [1,0] then doing a Â· b would be equivalent of saying is there a value at a[0] AND b[0], etc. \n",
    "\n",
    "Neural nets can also use partial weights to represent partial AND-ing. Very useful when modeling probabilities in neural networks. \n",
    "\n",
    "Finally, there are also negative weights, which imply a logical NOT operator. \n",
    "\n",
    "With this kind of logical mapping to our weights, we can almost develop a crude way to read these arrays. For example: \n",
    "```\n",
    "weights = [1, 0, 1] => if input[0] OR input[2]\n",
    "weights = [0, 0, 1] => if input[2]\n",
    "weights = [1, 0, -1] => if input[0] OR NOT input[2]\n",
    "weights = [-1, 0, -1] => if NOT input[0] OR NOT input[2]\n",
    "weights = [0.5, 0, 1] => if BIG input[0] OR input[2]\n",
    "```\n",
    "\n",
    "For the last example, the smaller weight means that the input at position 0 needs to be larger to have an effect on the score at the end. \n",
    "\n",
    "So, given these intuitions, we can posit that when our NNet is making a prediction, it really is telling us how similar our inputs are to our weights. But weights do not make the final decision regarding score, it is the combination of the value of a weight AND the value of an input that determine the impact on the final score. \n",
    "\n",
    "## Multiple Inputs - Complete Runnable Code\n",
    "\n",
    "Instead of using the clunky code we wrote in the beginning, there is a much simpler way to write the same code in numpy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.98\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "weights = np.array([0.1, 0.2, 0])\n",
    "def neural_network(input, weights):\n",
    "    return input.dot(weights)\n",
    "\n",
    "toes = np.array([8.5, 9.5, 9.9, 9.0]) \n",
    "wlrec = np.array([0.65, 0.8, 0.8, 0.9]) \n",
    "nfans = np.array([1.2, 1.3, 0.5, 1.0])\n",
    "\n",
    "input = np.array([toes[0], wlrec[0], nfans[0]])\n",
    "print neural_network(input, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a Prediction with Multiple Outputs\n",
    "> NNets can also make multiple predictions using only a single input\n",
    "\n",
    "A single input -> multiple output NNet is simple as well. \n",
    "```\n",
    "           Hurt? \n",
    "         /\n",
    "        /\n",
    "input - -  Win? \n",
    "        \\\n",
    "         \\\n",
    "           Sad? \n",
    "```\n",
    "\n",
    "This is not a 'dense' neural net. Notice that each prediction is completely separate from the other two. This makes this network easy to implement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.27, 0.18000000000000002, 0.81]\n"
     ]
    }
   ],
   "source": [
    "def neural_network(input, weights):\n",
    "    return list(map(lambda x: x * input, weights))\n",
    "\n",
    "weights = [0.3, 0.2, 0.9]\n",
    "wlrec = [0.9, 0.8, 0.8, 0.9]\n",
    "print neural_network(wlrec[0], weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting with Multiple Inputs AND Outputs (Dense Graph)\n",
    "> NNets can predict multiple outputs given multiple inputs\n",
    "\n",
    "Finally, we can combine what we've learned to create a graph / net where each input node is connected to each output prediction, with it's own weight attached. Furthermore, the steps also remain mostly the same: \n",
    "1. Insert one datapoint\n",
    "2. For each output, perform a weighted sum of Inputs\n",
    "3. Deposit Predictions\n",
    "\n",
    "How does it work? It performs 3 independent weighted sums of the input to make 3 predictions. \n",
    "Easier to think of this as 3 weights coming into each output node, and all these graphs are overlaid onto each other. Check out the code below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neural_network(input, weights):\n",
    "    return vect_mat_mul(input, weights)\n",
    "\n",
    "def vect_mat_mul(vect, matrix):\n",
    "    output = [0] * len(vect)\n",
    "    for i in range(len(vect)):\n",
    "        output[i] = w_sum(vect, matrix[i])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are more variables flying around here. We are choosing to think of this network as a series of weighted sums. In this code we created a helper function, vect_mat_mul, that iterates through each vector of 'weights', and makes a prediction using the w_sum (dot product) function. \n",
    "\n",
    "There are a few new terms introduced here as well, a list of vectors is called a **matrix**. Secondly, we will learn lots of functions that leverage matrices, the one used here is called **vector_matrix_multiplication**. Essentially our series of weighted sums. We take a vector, perform a dot product with every vector within the matrix. There are, as usual, special numpy functions that can help us out here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Predicting on Predictions\n",
    "\n",
    "**Neural networks can be stacked!** \n",
    "This is what 'deep' learning is. You stack multiple layers of learning into a network. Taking the output of one network and feeding it into the next network. Specifics coming later, but for now we can just know that it's possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.2135  0.145   0.2605]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "ih_wgt = np.array([\n",
    "    [0.1, 0.2, -0.1],\n",
    "    [-0.1, 0.1, 0.9],\n",
    "    [0.1, 0.4, 0.1],\n",
    "]).T\n",
    "\n",
    "hp_wgt = np.array([\n",
    "    [0.3, 1.1, -0.3],\n",
    "    [0.1, 0.2, 0.0],\n",
    "    [0.0, 1.3, -0.1],\n",
    "]).T\n",
    "\n",
    "weights = [ih_wgt, hp_wgt]\n",
    "\n",
    "def neural_network(input, weights):\n",
    "    hid = input.dot(weights[0])\n",
    "    pred = hid.dot(weights[1])\n",
    "    return pred\n",
    "\n",
    "toes = np.array([8.5, 9.5, 9.9, 9.0])\n",
    "wlrec = np.array([0.65, 0.8, 0.8, 0.9])\n",
    "nfans = np.array([1.2, 1.3, 0.5, 1.0])\n",
    "\n",
    "input = np.array([toes[0], wlrec[0], nfans[0]])\n",
    "pred = neural_network(input, weights)\n",
    "print pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
